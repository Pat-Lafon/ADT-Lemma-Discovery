The automation of our implementation and the final verification step in the workload which is verified by Dafny is one of the main concerns.  Initially we try to use data-driven method the simplify the verification of client program to verification of the lemmas, but it seems do not really solve the problem, but pass it to other tools. That is the point we should improve.

Another concern is about the application of our method, these lemmas can be predefined or the specification of the library can rich enough to fit any cases, or the user will try to write there own lemmas directly. One assumption of our problem definition, the specification of the library cannot fit  the client assertion, are also be doubted. We should try to prove this assumption using stronger example(specification from existing library, etc) paper. As one reviewer said, specification is a contract between the library and its clients. Then, "strengthen specification by inferred lemmas" seems not a good way, maybe we should change our application settings.

Reviewers also concerned about the error cases in our benchmarks including when the program is buggy or our tool infers a lemma which is not consistent with the predicates semantics. When the program is buggy, our algorithm will loop forever, and we cannot distinguish if there exists a lemma or the program or assertions are wrong. The lemmas are verified by other tools and we do not know if these tools will fail. We should try to handle the error cases here.

We are also willing to clarify some misunderstanding. One reviewer have questions about the difference between our "negative sample" and "bad input". The "sample" (defined in Definition 3.5) is informally a model in SMT, and this model can describe some impossible data(cannot be written in list literal) which violate the intended semantics of predicates. The range of "sample" is larger than data in other data-driven method. Actually we gave the example and explanation about why this happened. On paragraph from 302, the CEX is the negative sample, but it cannot be written as "bad input" as "These claims clearly violate the intended semantics of member and hd; it should not be possible for s1 to have a head which is not also a member."(line 311-313). No such a list instance consistent with this CEX, thus there is not a "input" the program can run. The data-driven method with an empty set of "bad" inputs will definitely generate something equivalent to "true", and that is exactly the problem we mentioned in this paper. The true they learned cannot solve the problem we mentioned in motivation example, where the problem is the spec of library is not match the client's VC and the verification failed.

There is also a misunderstanding about our feature set. Lines 833-840 state all feature vectors "consistent with each of them(the negative samples s-)", not simply all feature vectors. We said in lines 1172-1173, these feature vectors are "s possible in the hypothesis space", which is the size of the hypothesis space, not all of feature vectors will be gathered by the algorithm. The amount of feature vectors used by our algorithm is also listed in Table 5 and Table 6("V+" and "V-" column), the largest on is 18336. On Line 7 in Alg, each feature set is computed after we assignment the variables an concrete type, and summarize the combination of variables and predicates, and "AllFeatureSet" returns a set of feature sets, and the size of these feature sets are not too big. Line 752-759 give the some possible feature set, although there are 3 predicates, 3 variables and 2 possible types(list type and element type), the size of feature set is not larger than 6.

The related works mentioned by reviewers will also help us to improve our work.
