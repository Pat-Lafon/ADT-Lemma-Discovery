Reviewer A:

Comments:
1. The paper's premise is that we are given a fully specified client program but a partially specified library. I am not convinced that is a realistic or important setting.
  I think the "partially specified library" is an misunderstanding of what we talked about. The spec of library is not partial, the spec of library can be fullly limited but its body is using some predicates which defined inductivly. And the definition of the predicates are not in FOL, then the spec itself fails to verify some client code alone. Our lemma inference just try to fix this.

2. we cannot consider it truely verified until the lemmas themselves are also verified
  Our approach is trying to infer the missing lemmas, and our evaluation shows that these lemmas can be verified by existing tools. Our approach is better than using these tools(like Dyfny) directly as our data-driven inference stronggly simplified the problem. We also showed how large the lemma hypothesis space is, it is hard for the users to write their own.

3. suspicious that the evaluation states that the learned lemmas were always correct...
  Our lemma is checked by the property based sampler during the inference. When the discovered lemma is not consistent with the predicates semantics, it will be checked by the property based sampler, and the sampler will provide the counter-example. That counter-example will be added to the positive samples to improved in lemma. We describe this part on line 863-867 and one line 28-34 in our figure of algorithm. That is the reason they are always correct.
  The incorrect program will make us fail to find the lemma, we will increase the size of quantifed variables and loop forever. In pratical, we can set a bound for the size of quantified variables, but these test cases we believed is not interesting.

Related Works:
I didn't read them.

Unclarities
1. We don't limited the logic as the logic is not a barrier of our approach. The logic, which depends on the user choices, but at least support the EUF, as our predicates are uninterpreted(and they are basic). The use can use arithemetic(like our motivation example) and they can use the set operation in spec if they like. But we assumed they are deciable, as we need the SMT query results in our Alg. we also limited the lemmas in EPR are we do not what to break the decidability of original VCs. In our evaluation, we use arithemetic and EUF.

2. This is a misunderstanding. Lines 833-840 state all feature vectors "consistent with each of them(the negative samples s-)", not simply all feature vectors. We said in lines 1172-1173, these feature vectors are "s possible in the hypothesis space", which is the size of the hypothesis space, not all of feature vectors will be gethered by the algorithm. Actually, for an effiecient data-driven searching algorithm, even the possible data space is large, we donot need sample all possible samples to get the correct answer. The amount of feature vectors used by our algorithm is also listed in Table 5 and Table 6("V+" and "V-" column), the largest on is 18336.

3.talked above.

4. This is a misunderstanding. Line 7 the function "AllFeatureSet" does not compute an union of all sub feature sets, or combine all oppertunities of the type assignment of each variables. Each feature set is computed after we assignment the variables an concrete type, and summarize the combination of variables and predicates, and "AllFeatureSet" returns a set of feature sets. The loop on line 7 in Alg1 try each feature set, and the size of these feature set is not too big. In Table 5, we said the |F| is "e feature set necessary for Elrond to find a candidate lemm", that is the size of the "F" on line 7 which makes "DiscoverLemma" return lemma. Line 752-759 give the some possible feature set, although there are 3 predicates, 3 variabels and 2 possible types(list type and element type), the size of feature set is not larger than 6.

Reviewer B:

Main comments and questions
1. The data-driven method with an empty set of "bad" inputs will definitely generate something equivalent to "true", and that is exactly the problem we mentioned in this paper. The true they learned cannot solve the problem we mentioned in motivation example, where the problem is the spec of library is not match the client's VC and the verification failed. (I guess this reviewer does not follow what are we doing from the begining.)

2. The "negative" sample is not bad input at all. The "sample" (defined in Definition 3.5) is informally a model in SMT, and this model can describe some impossible data(cannot be written in list literal) which violate the intended semantics of predicates. The range of "sample" is larger than data in other data-driven method. Actually we gived the example and explaination about why this happened. On paragraph from 302, the CEX is the negative sample, but it cannot be written as "bad input" as "These claims clearly violate the intended semantics of member and hd; it should not be possible for s1 to have a head which is not also a member."(line 311-313). No such a list instance consistent with this CEX, thus there is not a "input" the program can run.

3. As mentioned, the PIE returns just true as there is not "bad input", we compared them on paragraph line 152.

4. When the discovered lemma is not consistent with the predicates semantics, it will be checked by the property based sampler, and the sampler will provide the counter-example. That counter-example will be added to the positive samples to improved in lemma. We describe this part on line 863-867 and one line 28-34 in our figure of algorithm.

Reviewer C:

Questionable applicability.
Dynfy is our implementation of our tool pipeline, to show that our method can help verify the program with many complicate data structures. Dynfy is not the main part of our contribution. After we get the lemma, there are many ways to verify if the lemma is consistent with this semantic, Dyfny is just one of them.

- Are the data structures directly implemented in Dafny or is the Dafny code obtained from the OCaml code?
In our implementation, we hard code all used data structure and the predicates directly in a Dafny header, but it is also not hard to build a translator to translate the definition of data structure in Ocaml to Dafny. I dont think it is relevant to the contribution of this paper.

- What is the level of automation of this process?
The whole process is automated. In Figure 4, we describe the process of our tool. We provide the client program and library code as input and the tool can generate the 1) spec 2) lemma automatically, and we guarantee the correctness of the program is reduced by the correctness of the lemmas. The final step we verifed the lemmas also using automatic tool, some of them faile, but they should be true(by adding annotation). These annotation is just try to show the lemmas we inferred can verified.

No support for mutable data structures
...

Unconvincing evaluation:
- Using a pre-computed library of lemmas about the analysed data-structure (which can themselves be inferred automatically).
  First of all, the data structure is fixed but the predicates are not. Even the spec of library uses a predicates set, but the assertion in client may introduce new predicates and this introduce new lemmas too. Even we limited the predicates, as we mentioned, the hypothsis space of the lemmas are large and the amount of correct lemmas are also large(on line 1170 - 1172), thus it is not pratical to use a pre-computed library of lemmas. And the lemmas may not fit the assertion required by the client, these we need infer them aim to prove the client assertion.

- Simply inlining the library code.
  The function may call other functions(and may recursively), it is hard to inline the code[BTW, how to inline functions in a functional language?]

Narrow Related work:
The imperitive language is quite different from functional langauge. 

