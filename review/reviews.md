PLDI 2021 Paper #169 Reviews and Comments
===========================================================================
Paper #169 Data-driven Lemma Discovery


Review #169A
===========================================================================

Overall merit
-------------
C. I would not accept this paper but will not argue strongly against
   accepting it.

Reviewer expertise
------------------
X. **Expert** = "I have written a paper on one or more of this paper's
   topics"

Paper summary
-------------
The paper considers verification of client programs that use libraries
for manipulating data structures based on algebraic data types.  The
paper's focus is a setting in which there exists some specification of
the library code, but it is insufficient to prove the client
program. The paper then suggests to learn the missing lemmas from
positive and negative examples, restricting to prenex form universally
quantified formulas, where the quantifier-free body is learned by
decision tree learning.  The negative examples come from
counterexamples to the VCs of client code, generated by an SMT solver.
The positive examples come from random tests generated by QuickCheck.
Thus, the learned lemmas are guaranteed to be sufficient to prove the
client code, but not guaranteed to be correct.  The authors evaluates
the approach on a corpus of OCaml programs, and also manually verify
that the learned lemmas are correct.

Comments for author
-------------------
The paper considers an important and challenging problem, and the data
driven approach and the results of the evaluation are
promising. However, I find some of the paper's assumptions and its
setting problematic (as detailed below). There are also some related
works not discussed by the paper that make its novelty more
limited. In general I found the paper easy to read, but there were
several unclarities that made it hard for me to understand what the
author's implementation really does.

## Assumptions and setting

* The paper's premise is that we are given a fully specified client
  program but a partially specified library. I am not convinced that
  is a realistic or important setting. Instead, I would expect a
  library of verified code to be fully specified, such that the
  modular specification serves as a contract between the library and
  its clients. That is, clients should be verified agains the provided
  specification, since the library implementation may change as long
  as it does not break the provided specification.

* The paper only uses random testing to ensure that the learned lemmas
  are correct. This means that while the discovered lemmas suffice to
  "verify" the client program, we cannot consider it truely verified
  until the lemmas themselves are also verified, which requires manual
  intervention and may render the approach impractical (as users may
  find it easier to come up with their own lemmas rather than manually
  verifying automatically generated ones).

* I find it suspicious that the evaluation states that the
  learned lemmas were always correct. This means that on the benchmark
  suite considered, random testing is empirically as good as formal
  verification. One reason for this may be that the authors did not
  experiment with incorrect client programs, which would occur in a
  realistic software development process.

## Related works

* [First-Order Quantified Separators, Koenig et al., PLDI 2020] presented a procedure to learn general quantified first-order formulas based on positive and negative samples that can be directly used to solve the problem considered by the submission.

* [Effectively Propositional Interpolants, Drews and Albarghouthi, CAV 2016] presented a procedure to learn universally quantified, existentially quantified, and alternation-free interpolants. As this procedure is model based, it can also be used to learn separators.

The paper would benefit from a comparison to both above works. As far
as I can tell, the main ingredient new in the paper's technique is to
greedily add all feature vectors from negative samples, and combine it
with DT learning. It would be interesting to compare this inductive
bias to the one in the above works, which have different inductive
biases. ([PLDI 2020] minimizes number of quantifiers and CNF size of
the body, and [CAV 2016] minimizes each clause individually.)

## Unclarities

* It was not clear to me what logic is used to check VCs. In several
  places, the paper mentions predicates are uninterpreted, giving the
  impression that it is the logic of equality and uninterpreted
  functions. However, line 298 as well as lines 835-837 refer to
  integer constants. Several places of the paper mention decidability
  and there is a mention of effectively propositional logic (line
  584), but I could not find a clear statement of what logic is used
  and if it is decidable.

* Lines 833-840 state that as part of the algorithm, there is an SMT
  solver query for every possible feature vector. However, line 1171
  and table 6 state that the algorithm is practical even with 2^40
  possible feature vectors, which I could not reconcile.

* The implementation and evaluation mention that the client code and
  library code is written in OCaml. However, the lemmas are reported
  to be verified using Dafny, which is a different language. Was this
  done by manually translating library code from OCaml to Dafny? The
  paper does not explain.

* The evaluation only shows examples where Elrond was successful. The
  paper is missing some examples where it fails and a discussion of
  the failure modes and restrictions of the approach.

* The sizes of the feature sets reported in Tables 5 and 6 are
  surprisingly small. Do these take into account the loop of Alg. 1
  line 7? That is, that there are multiple possibilities for the type
  of each variable? Even with just two types and 3 variables, this
  gives 8 different possibilities, which do not seem to be reflected
  in Tables 5 and 6.

## Minor comments

* line 215: themx -> them

* line 244: rephrase sentence

* Figure 3: how come the "Datatype Instance" area is not contained in the $\Sigma$ area? Does it not mean that $\Sigma$ is unsound if it is violated by some datatype instances? Also, if both green and orange areas are contained in $\Sigma$, they can simply be labeled as $\Phi$ and $\neg \Phi$.

* lines 648: isn't this exactly item 2 above? why is it "additionally"?

* lines 661: remove curly braces

* line 793 (Alg. 1 line 21): $\neq$ should be $=$



Review #169B
===========================================================================

Overall merit
-------------
C. I would not accept this paper but will not argue strongly against
   accepting it.

Reviewer expertise
------------------
X. **Expert** = "I have written a paper on one or more of this paper's
   topics"

Paper summary
-------------
This paper presents a new approach to client-side verification of software. Specifications for software libraries are often not incomplete for client-specific verification tasks. This work formalizes the modular client-side verification problem, describes  a data-driven technique for inferring lemmas regarding library functions, which would be sufficient for proving client-side correctness, presents an implementation ($\textsf{Elrond}$) of the proposed technique and evaluation over data structure implementations in OCaml.

Comments for author
-------------------
This paper targets an interesting problem. I think the proposed approach is described clearly overall with several examples throughout. My main concern is that the key contribution of this work --- learning a sufficiently strong lemma $L$ under which a client-side VCs $\Phi$ can be proved from a library specification $\Sigma$ --- is essentially a precondition inference problem, and I think the paper lacks a more thorough comparison of the proposed approach to existing data-driven precondition inference techniques.

### Main comments and questions

I have the following specific comments regarding the lemma discovery part:

1. [Lines 152 -- 164] The authors mention that existing data-driven methods timeout when asked to synthesize sufficient lemmas, because they require "good" and "bad" inputs and in this case there are no "bad" inputs. With an empty set of "bad" inputs, wouldn't existing approaches instantly generate something equivalent to "true"?

2. Also related to the above point about "bad" inputs -- can the "negative" samples (described in Section 2.1 and shown in Figure 3) be treated as "bad" inputs? I would encourage the authors to add some explanation on exactly how these "negative" samples differ from "bad" inputs required by prior work.

3. [Lines 367 -- 376] Lemma discovery is posed as an abductive inference problem -- find $L$ such that $L \land \Sigma \implies \Phi$ holds. It can also been seen a precondition inference problem -- find a precondition $L$ under which $\Sigma \implies \Phi$ holds, i.e. $L \implies (\Sigma \implies \Phi)$.  
The lemma discovery procedure seems very closely related to the automatic feature-learning based precondition inference algorithms presented in prior work [19], and I think the paper is missing a thorough comparison with the prior work in this context.

4. [Section 3.3] If the discovered lemma is deemed not be a true lemma with respect to the predicates' semantics, is there a way to improve the candidate lemma, may be using counterexamples from the verifier? If so, how exactly would the counterexamples be used? I would encourage the authors to add more details regarding whether this is possible and how it could be implemented.

### Other minor comments

- [Line 1094] "... $\textsf{Elrond}$is ...": missing space

- [Line 1087] "... the shape postcondition impact ...": missing "of"



Review #169C
===========================================================================

Overall merit
-------------
C. I would not accept this paper but will not argue strongly against
   accepting it.

Reviewer expertise
------------------
Y. **Knowledgeable** = "I follow the literature on this paper's topic(s)
   but may have missed some relevant developments"

Paper summary
-------------
The paper proposes a new data-driven lemma discovery mechanism for modularly proving clients of data structure libraries. The proposed lemma discovery mechanism is meant to automatically infer the properties of the libraries required for proving the correctness of the client program being analysed. These properties are usually non-trivial as data structures are typically inductive. 

The proposed approach uses a CEGIS-style inference procedure to drive the verification of client programs. More concretely, the authors treat the library primitives (e.g. hd and member of a list) as uninterpreted functions and use an SMT solver to generate infeasible interpretations of these primitives during the verification process. They then combine these infeasible interpretations with feasible interpretations obtained by concretely running the library on randomly generated test data to obtain a "dataset" describing the missing property. Using the obtained "dataset", the authors then construct a new formula describing the required property with the help of the C4.5 decision tree algorithm. This process is repeated either until enough lemmas are generated for the verification of the client to go through or until the inference procedure is unable to generate a new candidate lemma. 

To illustrate their approach, the authors implement a prototype of their lemma discovery mechanism as part of a verification tool for OCaml and use the obtained tool to verify a number of client programs that make use a range of functional data structure libraries, including search trees, tries, heaps, lists, stacks, queues, and sets.

Comments for author
-------------------

Pros: 
- Simple elegant technique 
- Didactical running example

Cons: 
- Questionable applicability (in current state)
- No support for mutable data structures 
- Unconvincing evaluation 
- Narrow related work section

**Questionable applicability** 
As I understand, the goal of the proposed procedure is to enable the *automatic* modular verification of data-structure clients. So, in my view, it is essential that the obtained lemmas be verified against their respective library code. As part of the evaluation of the tool, the authors use Dafny to validate the generated lemmas. However, no details are given wrt that process. One is left wondering: 
- Are the data structures directly implemented in Dafny or is the Dafny code obtained from the OCaml code? 
- What is the level of automation of this process? 

The authors do state that they had to manually annotate around 1/3 of the Dafny programs with inductive invariants for their verification to go through. This, in my opinion, defeats the purpose of the entire approach. Instead of proving their own lemmas, users must prove automatically generated lemmas.

**No support for mutable data structures**
The proposed procedure does not support mutable data structures and the automatic 
inference of loop invariants. These limitations severely constrain the applicability of the tool as  presented in the paper, given that, even in OCaml, most data structures are written in imperative style (see, for instance, "Designing a Generic Graph Library Using ML Functors" by Conchon et al). 

**Unconvincing evaluation**
The authors evaluate their tool on a small number of library clients, showing that it generates the required lemmas for verification to go through. Ideally, the evaluation of the paper should show that the proposed technique is more effective than: 
- Using a pre-computed library of lemmas about the analysed data-structure (which can themselves be inferred automatically); 
- Simply inlining the library code. 


**Narrow Related work**
There is an large body of works on the verification of sequential data structure libraries and their clients. The related work section focusses on a small number of papers immediately related to the proposed approach, ignoring important references on topics such as bi-abduction and automatic inference of inductive predicates: 
- Compositional shape analysis by means of bi-abduction by O'Hearn et al
- Automatic induction proofs of data-structures in imperative programs by Jaffar et al
- A generic cyclic theorem prover by Brotherston et al. 


Typos:  
- ln 291: The universally quantified variable v does not seem to be in the correct font 
- ln 434: "are propositions whose literals are." -> incomplete sentence 
- ln 849: "on line" -> on lineS
- ln 1146: repeated parenthesis



Response by Zhe Zhou <zhou956@purdue.edu>
---------------------------------------------------------------------------
Thank you for your detailed comments.  We will incorporate your suggestions regarding problem setup, related work, and evaluation in our revision.

It is clear from the reviews that there is skepticism with several aspects of our problem setup, particularly the assumption that we only have incomplete specifications of library methods and the need to manually verify inferred lemmas in order to have a complete verification story. There was also concern that our evaluation only involved correct programs and valid inferred lemmas. We plan to further refine our problem setup and to include buggy programs and invalid inferred lemmas in subsequent iterations of the paper.

We would also like to clarify a couple of specific points raised by the reviewers:

Q: No experiment with incorrect client programs? (Reviewer A)

A: As with other abductive mechanisms, our approach seeks a lemma consistent with the library definitions sufficient to imply a client post-condition.  Programs with bugs, either in the library or client code, will likely result in the algorithm exploring a potentially infinite hypothesis space, since it cannot distinguish whether a postulated lemma failed because it was too weak, or the program or assertions were wrong.

Q: Difference between negative samples and bad inputs (Reviewer B)?

A: A "sample" (defined in Definition 3.5) is informally a model in SMT,
and this model can describe impossible to construct data that violates
the intended semantics of its constituent predicates.  In line 302,
the CEX is a negative sample, but it cannot be written as a "bad
input" since "These claims clearly violate the intended semantics of
member and hd; it should not be possible for s1 to have a head which
is not also a member."(line 311-313)." In other words, a negative
sample is an interpretation of predicates made by the underlying
solver that cannot be witnessed by any concrete input.

Simply observing positive ("good") inputs, ascribing bad inputs
as "true", does not solve the problem since the missing lemma cannot
be derived from simply observing positive runs.
